{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f8f1191-3701-4694-bbe3-7a2b5bccf281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting c1sr.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile c1sr.py\n",
    "import asyncio\n",
    "import streamlit as st\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, pipeline\n",
    "import noisereduce as nr\n",
    "import soundfile as sf\n",
    "import pvporcupine\n",
    "import pyaudio\n",
    "import struct\n",
    "from googletrans import Translator\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "import time\n",
    "\n",
    "# Custom Styling with Background and Fonts\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "    <style>\n",
    "        body {\n",
    "            background-color: #121212;\n",
    "            color: white;\n",
    "            font-family: Arial, sans-serif;\n",
    "        }\n",
    "        .stTitle {\n",
    "            font-size: 36px;\n",
    "            font-weight: bold;\n",
    "            color: #ff4b4b;\n",
    "            text-align: center;\n",
    "        }\n",
    "        .stSidebar {\n",
    "            background-color: #1e1e1e;\n",
    "            color: white;\n",
    "        }\n",
    "        .stButton>button {\n",
    "            background-color: #ff4b4b;\n",
    "            color: white;\n",
    "            border-radius: 8px;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "# Sidebar for Navigation\n",
    "st.sidebar.markdown(\"<h1 style='color:#ff4b4b;'>TransTalk</h1>\", unsafe_allow_html=True)\n",
    "st.sidebar.image(\"C:/Users/Pranjal Oza/Downloads/human_13852054.png\", width=100)\n",
    "st.sidebar.markdown(\"### Options\", unsafe_allow_html=True)\n",
    "input_method = st.sidebar.radio(\"Choose Input Method\", [\"Upload Audio File\", \"Upload Video File\", \"Live Audio\"])\n",
    "\n",
    "# Load Sentiment & Emotion Analysis Models\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", framework=\"pt\")\n",
    "emotion_analyzer = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", framework=\"pt\")\n",
    "translator = Translator()\n",
    "\n",
    "def load_audio(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    waveform = resampler(waveform.mean(dim=0, keepdim=True))\n",
    "    return waveform.squeeze().numpy(), 16000\n",
    "\n",
    "def reduce_noise(audio, rate):\n",
    "    return nr.reduce_noise(y=audio, sr=rate)\n",
    "\n",
    "def extract_audio_from_video(video_path, output_audio_path=\"extracted_audio.wav\"):\n",
    "    try:\n",
    "        video = VideoFileClip(video_path)\n",
    "        if video.audio is None:\n",
    "            raise ValueError(\"No audio found in the video.\")\n",
    "        video.audio.write_audiofile(output_audio_path)\n",
    "        return output_audio_path\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error extracting audio: {e}\")\n",
    "        return None\n",
    "\n",
    "def recognize_live_speech():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        st.write(\"üé§ Listening...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio_data = recognizer.listen(source)\n",
    "        with open(\"live_audio.wav\", \"wb\") as f:\n",
    "            f.write(audio_data.get_wav_data())\n",
    "        return \"live_audio.wav\"\n",
    "\n",
    "# Keyword Spotting (Wake Word Detection)\n",
    "def detect_wake_word(timeout=20):\n",
    "    start_time = time.time()\n",
    "    porcupine = pvporcupine.create(access_key=\"G01a+afHqzWPIh74GrxkuOKYq3QSNrlehhyi08XHqYjZvbSdh62FMA==\", keyword_paths=[\"C:/Users/Pranjal Oza/Downloads/Hey-Marvel_en_windows_v3_0_0/Hey-Marvel_en_windows_v3_0_0.ppn\"])\n",
    "    pa = pyaudio.PyAudio()\n",
    "    stream = pa.open(rate=porcupine.sample_rate, channels=1, format=pyaudio.paInt16, input=True, frames_per_buffer=porcupine.frame_length)\n",
    "\n",
    "    st.write(\"Listening for wake word... (Timeout in 20 seconds)\")\n",
    "    while time.time() - start_time < timeout:\n",
    "        pcm = stream.read(porcupine.frame_length)\n",
    "        pcm = struct.unpack_from(\"h\" * porcupine.frame_length, pcm)\n",
    "        keyword_index = porcupine.process(pcm)\n",
    "        if keyword_index >= 0:\n",
    "            st.write(\"Wake word detected!\")\n",
    "            break\n",
    "    else:\n",
    "        st.error(\"Wake word not detected. Please try again.\")\n",
    "    \n",
    "    stream.close()\n",
    "    pa.terminate()\n",
    "\n",
    "def transcribe_audio(audio_file, language=\"en\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\").to(device)\n",
    "    \n",
    "    audio, rate = load_audio(audio_file)\n",
    "    audio_denoised = reduce_noise(audio, rate)\n",
    "    input_features_denoised = processor(audio_denoised, return_tensors=\"pt\", sampling_rate=16000).input_features.to(device)\n",
    "    forced_decoder_ids = processor.tokenizer.get_decoder_prompt_ids(language=language, task=\"transcribe\")\n",
    "    \n",
    "    generated_ids_denoised = model.generate(input_features_denoised, forced_decoder_ids=forced_decoder_ids)\n",
    "    transcription_denoised = processor.batch_decode(generated_ids_denoised, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return transcription_denoised\n",
    "\n",
    "audio_file = None\n",
    "\n",
    "if input_method == \"Upload Audio File\":\n",
    "    uploaded_audio = st.file_uploader(\"üìÇ Upload an audio file\", type=[\"wav\", \"mp3\"])\n",
    "    if uploaded_audio:\n",
    "        audio_file = \"uploaded_audio.wav\"\n",
    "        with open(audio_file, \"wb\") as f:\n",
    "            f.write(uploaded_audio.read())\n",
    "\n",
    "elif input_method == \"Upload Video File\":\n",
    "    uploaded_video = st.file_uploader(\"üìπ Upload a video file\", type=[\"mp4\", \"avi\"])\n",
    "    if uploaded_video:\n",
    "        video_file = \"uploaded_video.mp4\"\n",
    "        with open(video_file, \"wb\") as f:\n",
    "            f.write(uploaded_video.read())\n",
    "        extracted_audio = extract_audio_from_video(video_file)\n",
    "        if extracted_audio:\n",
    "            audio_file = extracted_audio\n",
    "\n",
    "\n",
    "elif input_method == \"Live Audio\":\n",
    "    detect_wake_word()\n",
    "    audio_file = recognize_live_speech()\n",
    "\n",
    "if audio_file:\n",
    "    with st.spinner(\"‚è≥ Processing audio...\"):\n",
    "        transcribed_text_denoised = transcribe_audio(audio_file)\n",
    "\n",
    "    # Save transcription to a text file\n",
    "    with open(\"transcription.txt\", \"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(transcribed_text_denoised)\n",
    "    st.success(\"‚úÖ Transcription saved to `transcription.txt`.\")\n",
    "    \n",
    "    if transcribed_text_denoised.strip():\n",
    "        st.markdown(\"## üéôÔ∏è Transcribed Text\", unsafe_allow_html=True)\n",
    "        st.success(transcribed_text_denoised)\n",
    "        \n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            sentiment_result = sentiment_analyzer(transcribed_text_denoised)[0]\n",
    "            st.markdown(f\"**Sentiment:** `{sentiment_result['label']}`\")\n",
    "        with col2:\n",
    "            emotion_result = emotion_analyzer(transcribed_text_denoised)[0]\n",
    "            st.markdown(f\"**Emotion:** `{emotion_result['label']}`\")\n",
    "            \n",
    "            emotion_gif = {\n",
    "                \"joy\": \"https://media.giphy.com/media/fPRwBcYd71Lox1v7p2/giphy.gif\",\n",
    "                \"anger\": \"https://media.giphy.com/media/3o6Zt481isNVuQI1l6/giphy.gif\",\n",
    "                \"sadness\": \"https://media.giphy.com/media/d2lcHJTG5Tscg/giphy.gif\",\n",
    "                \"fear\": \"https://media.giphy.com/media/l2JehQ2GitHGdVG9y/giphy.gif\",\n",
    "                \"surprise\": \"https://media.giphy.com/media/1BXa2alBjrCXC/giphy.gif\"\n",
    "            }\n",
    "\n",
    "\n",
    "            gif_url = emotion_gif.get(emotion_result['label'].lower(), \"https://media.giphy.com/media/3o6ZsYRFmeIUbfnBfy/giphy.gif\")\n",
    "            st.markdown(f'<img src=\"{gif_url}\" width=\"200\" alt=\"emotion gif\">', unsafe_allow_html=True)\n",
    "\n",
    "        \n",
    "        target_lang = st.text_input(\"üåç Enter target language (e.g., 'es' for Spanish, 'fr' for French)\", \"es\")\n",
    "        translated_text = translator.translate(transcribed_text_denoised, dest=target_lang).text\n",
    "        st.markdown(\"## üåê Translated Text\", unsafe_allow_html=True)\n",
    "        st.info(translated_text)\n",
    "    else:\n",
    "        st.error(\"‚ùå No transcribed text found. Please try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efcd4a30-fad5-4016-9cb5-610e6a94a93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting c2sr.py\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import streamlit as st\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, pipeline\n",
    "import noisereduce as nr\n",
    "import soundfile as sf\n",
    "import pvporcupine\n",
    "import pyaudio\n",
    "import struct\n",
    "from googletrans import Translator\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "import time\n",
    "\n",
    "# Custom Styling for Light Theme with Animations and Hover Effects\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "    <style>\n",
    "        html, body, [class*=\"css\"] {\n",
    "            background-color: #fdfdfd;\n",
    "            color: #212529;\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            transition: background-color 0.3s ease-in-out;\n",
    "        }\n",
    "        .stTitle {\n",
    "            font-size: 36px;\n",
    "            font-weight: bold;\n",
    "            color: #007acc;\n",
    "            text-align: center;\n",
    "            animation: fadeIn 1.5s ease-in;\n",
    "        }\n",
    "        .stSidebar {\n",
    "            background-color: #ffffff;\n",
    "            color: #212529;\n",
    "        }\n",
    "        .stButton>button {\n",
    "            background-color: #007acc;\n",
    "            color: white;\n",
    "            border-radius: 8px;\n",
    "            padding: 0.5rem 1rem;\n",
    "            font-weight: 600;\n",
    "            transition: background-color 0.3s ease-in-out;\n",
    "        }\n",
    "        .stButton>button:hover {\n",
    "            background-color: #005fa3;\n",
    "            transform: scale(1.02);\n",
    "        }\n",
    "        .stTextInput>div>input {\n",
    "            border-radius: 8px;\n",
    "            border: 1px solid #ced4da;\n",
    "            padding: 0.5rem;\n",
    "            transition: border-color 0.3s ease;\n",
    "        }\n",
    "        .stTextInput>div>input:focus {\n",
    "            border-color: #007acc;\n",
    "            outline: none;\n",
    "        }\n",
    "        .stMarkdown h2 {\n",
    "            color: #007acc;\n",
    "            margin-top: 1.5rem;\n",
    "            animation: slideUp 0.8s ease-in-out;\n",
    "        }\n",
    "        @keyframes fadeIn {\n",
    "            from { opacity: 0; }\n",
    "            to { opacity: 1; }\n",
    "        }\n",
    "        @keyframes slideUp {\n",
    "            from { transform: translateY(20px); opacity: 0; }\n",
    "            to { transform: translateY(0); opacity: 1; }\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "# Sidebar for Navigation\n",
    "st.sidebar.markdown(\"<h1 style='color:#007acc;'>TransTalk</h1>\", unsafe_allow_html=True)\n",
    "st.sidebar.image(\"C:/Users/Pranjal Oza/Downloads/human_13852054.png\", width=100)\n",
    "st.sidebar.markdown(\"### Options\", unsafe_allow_html=True)\n",
    "input_method = st.sidebar.radio(\"Choose Input Method\", [\"Upload Audio File\", \"Upload Video File\", \"Live Audio\", \"Record from Webcam\"])\n",
    "\n",
    "# Load Sentiment & Emotion Analysis Models\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", framework=\"pt\")\n",
    "emotion_analyzer = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", framework=\"pt\")\n",
    "translator = Translator()\n",
    "\n",
    "def record_video_from_webcam(duration=5, output_file=\"webcam_video.avi\"):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_file, fourcc, 20.0, (640, 480))\n",
    "    start_time = time.time()\n",
    "    while int(time.time() - start_time) < duration:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            out.write(frame)\n",
    "            cv2.imshow('Recording...', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return output_file\n",
    "\n",
    "def extract_audio_from_video(video_path, output_audio_path=\"extracted_audio.wav\"):\n",
    "    try:\n",
    "        video = VideoFileClip(video_path)\n",
    "        if video.audio is None:\n",
    "            raise ValueError(\"No audio found in the video.\")\n",
    "        video.audio.write_audiofile(output_audio_path)\n",
    "        return output_audio_path\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error extracting audio: {e}\")\n",
    "        return None\n",
    "\n",
    "def transcribe_audio(audio_file, language=\"en\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\").to(device)\n",
    "    waveform, sample_rate = torchaudio.load(audio_file)\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    waveform = resampler(waveform.mean(dim=0, keepdim=True))\n",
    "    audio_denoised = nr.reduce_noise(y=waveform.squeeze().numpy(), sr=16000)\n",
    "    input_features = processor(audio_denoised, return_tensors=\"pt\", sampling_rate=16000).input_features.to(device)\n",
    "    forced_decoder_ids = processor.tokenizer.get_decoder_prompt_ids(language=language, task=\"transcribe\")\n",
    "    generated_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "audio_file = None\n",
    "\n",
    "if input_method == \"Record from Webcam\":\n",
    "    st.info(\"Recording from webcam for 5 seconds...\")\n",
    "    video_path = record_video_from_webcam()\n",
    "    audio_file = extract_audio_from_video(video_path)\n",
    "\n",
    "if audio_file:\n",
    "    with st.spinner(\"‚è≥ Processing audio...\"):\n",
    "        transcribed_text = transcribe_audio(audio_file)\n",
    "\n",
    "    with open(\"transcription.txt\", \"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(transcribed_text)\n",
    "    st.success(\"‚úÖ Transcription saved to `transcription.txt`.\")\n",
    "\n",
    "    if transcribed_text.strip():\n",
    "        st.markdown(\"## üéôÔ∏è Transcribed Text\", unsafe_allow_html=True)\n",
    "        st.success(transcribed_text)\n",
    "\n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            sentiment_result = sentiment_analyzer(transcribed_text)[0]\n",
    "            st.markdown(f\"**Sentiment:** `{sentiment_result['label']}`\")\n",
    "        with col2:\n",
    "            emotion_result = emotion_analyzer(transcribed_text)[0]\n",
    "            st.markdown(f\"**Emotion:** `{emotion_result['label']}`\")\n",
    "\n",
    "            emotion_gif = {\n",
    "                \"joy\": \"https://media.giphy.com/media/fPRwBcYd71Lox1v7p2/giphy.gif\",\n",
    "                \"anger\": \"https://media.giphy.com/media/3o6Zt481isNVuQI1l6/giphy.gif\",\n",
    "                \"sadness\": \"https://media.giphy.com/media/d2lcHJTG5Tscg/giphy.gif\",\n",
    "                \"fear\": \"https://media.giphy.com/media/l2JehQ2GitHGdVG9y/giphy.gif\",\n",
    "                \"surprise\": \"https://media.giphy.com/media/1BXa2alBjrCXC/giphy.gif\"\n",
    "            }\n",
    "\n",
    "            gif_url = emotion_gif.get(emotion_result['label'].lower(), \"https://media.giphy.com/media/3o6ZsYRFmeIUbfnBfy/giphy.gif\")\n",
    "            st.markdown(f'<img src=\"{gif_url}\" width=\"200\" alt=\"emotion gif\">', unsafe_allow_html=True)\n",
    "\n",
    "        target_lang = st.text_input(\"üåç Enter target language (e.g., 'es' for Spanish, 'fr' for French)\", \"es\")\n",
    "        translated_text = translator.translate(transcribed_text, dest=target_lang).text\n",
    "        st.markdown(\"## üåê Translated Text\", unsafe_allow_html=True)\n",
    "        st.info(translated_text)\n",
    "    else:\n",
    "        st.error(\"‚ùå No transcribed text found. Please try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab74a0-c361-4cd8-8f4c-4384aaab4df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
